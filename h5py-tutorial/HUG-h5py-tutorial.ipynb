{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('py38': venv)",
   "display_name": "Python 3.8.6 64-bit ('py38': venv)",
   "metadata": {
    "interpreter": {
     "hash": "d64cb8562bf0829eb701d9c03e7f8a1e65280ba184316c37d8740918a5ad03a3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# HDF5 Users Group 2020 `h5py` Tutorial\n",
    "\n",
    "This is a very short tutorial about h5py, the Python language interface to the HDF5 library. Familiarity with the HDF5 [data model](https://portal.hdfgroup.org/display/HDF5/HDF5+User+Guides), its [programming API](https://portal.hdfgroup.org/pages/viewpage.action?pageId=50073943), and [NumPy](https://numpy.org/doc/stable/) is assumed.\n",
    "\n",
    "h5py resources:\n",
    "* GitHub [repository](https://github.com/h5py/h5py)\n",
    "* [Documentation](https://docs.h5py.org/en/2.10.0/)\n",
    "* O'Reilly [book](https://www.oreilly.com/library/view/python-and-hdf5/9781491944981/) written by h5py's creator\n",
    "\n",
    "This tutorial covers some of the most frequent HDF5 operations:\n",
    "\n",
    "1. Create a file, its groups and attributes.\n",
    "1. Create compact, chunked, resizable, and compressed HDF5 datasets.\n",
    "1. Open a file for reading and list group content.\n",
    "1. Read datasets and attributes.\n",
    "1. h5py low-level API and its relation to the HDF5 C API."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## Software Used in this Tutorial"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Python version:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.8.6 (default, Sep 28 2020, 04:40:29) \n[Clang 9.1.0 (clang-902.0.39.2)]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "source": [
    "Package versions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "h5py -> v2.10.0\nnumpy -> v1.19.2\n"
     ]
    }
   ],
   "source": [
    "for _ in (h5py, np):\n",
    "    print(f'{_.__name__} -> v{_.__version__}')"
   ]
  },
  {
   "source": [
    "_Note on h5py: The version used in this tutorial is the current latest release from more than a year ago. A new version, tagged 3.0, is being prepared and may be released soon._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "HDF5 library version:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5py.version.hdf5_version"
   ]
  },
  {
   "source": [
    "## Creating HDF5 Files, Groups, and Attributes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "An HDF5 file is created with the [`h5py.File`](https://docs.h5py.org/en/2.10.0/high/file.html) class:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 file \"hug-tutorial.h5\" (mode r+)>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "f = h5py.File('hug-tutorial.h5', mode='w')\n",
    "f"
   ]
  },
  {
   "source": [
    "Use `mode='r+'` if modifying already existing HDF5 file. Consult the [documentation](https://docs.h5py.org/en/2.10.0/high/file.html#reference) for all available keyword arguments to control properties of the created file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Some basic information about the created HDF5 file and the library settings that apply to it:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hug-tutorial.h5'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "f.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'sec2'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "f.driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('earliest', 'v110')"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "f.libver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "f.name"
   ]
  },
  {
   "source": [
    "The `name` property reveals the HDF5 [path name](https://portal.hdfgroup.org/display/HDF5/HDF5+Glossary#HDF5Glossary-P) of h5py objects. In the above case, the `name` value is `/` which means that `h5py. File` objects also represent HDF5 root group. We can use the file object to add other HDF5 content to the root group. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 group \"/g1\" (0 members)>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "g1 = f.create_group('g1')\n",
    "g1"
   ]
  },
  {
   "source": [
    "It is possible to create HDF5 groups at an arbitrary sublevel from the root group directly:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 group \"/several/levels/below\" (0 members)>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "g2 = f.create_group('several/levels/below')\n",
    "g2"
   ]
  },
  {
   "source": [
    "HDF5 groups in the file are available as [`h5py.Group`]() objects and their interface resembles Python dictionaries. For example, the `len()` function reports the number of member HDF5 objects in the group:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "len(f)"
   ]
  },
  {
   "source": [
    "`keys()`, `values()`, or `items()` methods return appropriate Python iterators for the group's HDF5 objects:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['g1', 'several']\n",
      "[<HDF5 group \"/g1\" (0 members)>, <HDF5 group \"/several\" (1 members)>]\n",
      "[('g1', <HDF5 group \"/g1\" (0 members)>), ('several', <HDF5 group \"/several\" (1 members)>)]\n"
     ]
    }
   ],
   "source": [
    "print(list(f.keys()))\n",
    "print(list(f.values()))\n",
    "print(list(f.items()))"
   ]
  },
  {
   "source": [
    "Working with HDF5 attributes is available from the `attrs` property of the h5py group and dataset objects. It exposes the [`AttributeManager` class](https://docs.h5py.org/en/2.10.0/high/attr.html#AttributeManager) which also has a dictionary-style interface.\n",
    "\n",
    "Creating HDF5 attributes is as simple as assigning new dictionary values. Below we create two HDF5 atrributes in the root group:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.attrs['attribute example 1'] = 'Example 1'\n",
    "f.attrs['attribute example 2'] = 2.0"
   ]
  },
  {
   "source": [
    "The `len()` function gives the number of assigned attributes:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "len(f.attrs)"
   ]
  },
  {
   "source": [
    "And `keys()`, `values()`, or `items()` methods all provide appropriate attributes information:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['attribute example 1', 'attribute example 2']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "list(f.attrs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Example 1', 2.0]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "list(f.attrs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('attribute example 1', 'Example 1'), ('attribute example 2', 2.0)]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "list(f.attrs.items())"
   ]
  },
  {
   "source": [
    "Use the [`create()`]() method for total control over all aspects of attribute creation. The default h5py behaviour is to delete the old attribute when creating new attribute with the same name. The [`modify()`](https://docs.h5py.org/en/2.10.0/high/attr.html#AttributeManager.modify) method should be used to only change the attribute's value. This means that the new value should be compatible with the properties of the old attribute value.\n",
    "\n",
    "Let's change the `attribute example 2` attribute's value to `15`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.attrs.modify('attribute example 2', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('attribute example 1', 'Example 1'), ('attribute example 2', 15.0)]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "list(f.attrs.items())"
   ]
  },
  {
   "source": [
    "and try to change its value to a string now:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Old attribute value datatype: float64\nERROR: No conversion path for dtype: dtype('<U12')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Old attribute value datatype: {f.attrs['attribute example 2'].dtype}\")\n",
    "    f.attrs.modify('attribute example 2', 'string value')\n",
    "except Exception as e:\n",
    "    print(f'ERROR: {e}')"
   ]
  },
  {
   "source": [
    "## Creating HDF5 Datasets\n",
    "\n",
    "HDF5 datasets constitute the _data payload_ in HDF5 files and are represented by the [`h5py.Dataset`](https://docs.h5py.org/en/2.10.0/high/dataset.html) class. [`h5py.Group.create_dataset()`](https://docs.h5py.org/en/stable/high/group.html#Group.create_dataset) method creates a new HDF5 dataset. Check its [documentation](https://docs.h5py.org/en/2.10.0/high/group.html#Group.create_dataset) for all available creation settings.\n",
    "\n",
    "To create a [contiguous](https://portal.hdfgroup.org/display/HDF5/HDF5+Glossary#HDF5Glossary-C) dataset in the group `g1`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 dataset \"contiguous\": shape (10,), type \"<i4\">"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "cont = g1.create_dataset('contiguous', data=np.arange(10), dtype='i4')\n",
    "cont"
   ]
  },
  {
   "source": [
    "Use the `chunks` argument to create a [chunked](https://portal.hdfgroup.org/display/HDF5/HDF5+Glossary#HDF5Glossary-C) dataset:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 dataset \"chunked\": shape (100, 200), type \"<i8\">"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "chunked = f.create_dataset('chunked', data=np.arange(100*200),\n",
    "                           shape=(100, 200), chunks=(20, 10))\n",
    "chunked"
   ]
  },
  {
   "source": [
    "Finding the \"right\" chunk size is the perennial issue without \"one size fits all\" answer. The HDF Group has developed many resources to explain chunking and its effect on I/O performance, see [this slide deck](https://www.slideshare.net/HDFEOS/hdf5-eosxiiiadvancedchunking) for one.\n",
    "\n",
    "Chunked datasets can also be resized or have _unlimited_ size (up to the maximum dimension size HDF5 allows). Both can be achieved by using the `maxshape` argument. To create a resizable dataset:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 dataset \"resizeable\": shape (10, 20), type \"<f4\">"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "resize = g2.create_dataset('resizeable', shape=(10, 20), maxshape=(200, 150))\n",
    "resize"
   ]
  },
  {
   "source": [
    "Setting any of the `maxshape`'s tuple elements to `None` declares that dataset's dimension as unlimited so its size can grow up to the maximum currently supported by HDF5: $2^{64}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 dataset \"unlimited\": shape (34, 76), type \"<f8\">"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "unlimited = f['several/levels'].create_dataset('unlimited', shape=(34, 76), maxshape=(None, 500), dtype=np.float64)\n",
    "unlimited"
   ]
  },
  {
   "source": [
    "Assigning attributes to datasets follows the same approach as for groups. To add attribute named `description` to a few already created datasets:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked.attrs['description'] = 'Chunked dataset'\n",
    "resize.attrs['description'] = 'Resizeable dataset'\n",
    "unlimited.attrs['description'] = 'Unlimited dataset'"
   ]
  },
  {
   "source": [
    "### Data Compression\n",
    "\n",
    "Data compression can be applied to any chunked HDF5 dataset. Compression method and its settings are set invidually for each dataset. Compression is only one of the ways how dataset data can be [manipulated](https://portal.hdfgroup.org/display/HDF5/Dynamic+Plugins+in+HDF5) when read or written.\n",
    "\n",
    "The `compression` argument defines the compression method and the `compression_opts` provides specific compression settings:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 dataset \"compressed\": shape (250, 300), type \"<f4\">"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "comp = f['several'].create_dataset('compressed', shape=(250, 300), chunks=(50, 60), \n",
    "                                   dtype='f4', compression='gzip')\n",
    "comp"
   ]
  },
  {
   "source": [
    "More information about available compression methods in h5py and their settings is available from its [documentation](https://docs.h5py.org/en/2.10.0/high/dataset.html#filter-pipeline)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Close the HDF5 file to flush all its content to storage:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "source": [
    "## Reading HDF5 Content\n",
    "\n",
    "We will now show how to read what was stored in the file we created in this tutorial. First, let's display the file's content using HDF5 command-line tools:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HDF5 \"hug-tutorial.h5\" {\nFILE_CONTENTS {\n group      /\n attribute  /attribute example 1\n attribute  /attribute example 2\n dataset    /chunked\n attribute  /chunked/description\n group      /g1\n dataset    /g1/contiguous\n group      /several\n dataset    /several/compressed\n group      /several/levels\n group      /several/levels/below\n dataset    /several/levels/below/resizeable\n attribute  /several/levels/below/resizeable/description\n dataset    /several/levels/unlimited\n attribute  /several/levels/unlimited/description\n }\n}\n"
     ]
    }
   ],
   "source": [
    "!h5dump -n 1 hug-tutorial.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/                        Group\n/chunked                 Dataset {100, 200}\n/g1                      Group\n/g1/contiguous           Dataset {10}\n/several                 Group\n/several/compressed      Dataset {250, 300}\n/several/levels          Group\n/several/levels/below    Group\n/several/levels/below/resizeable Dataset {10/200, 20/150}\n/several/levels/unlimited Dataset {34/Inf, 76/500}\n"
     ]
    }
   ],
   "source": [
    "!h5ls -r hug-tutorial.h5"
   ]
  },
  {
   "source": [
    "Open the file for reading only:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 file \"hug-tutorial.h5\" (mode r)>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "f = h5py.File('hug-tutorial.h5', mode='r')\n",
    "f"
   ]
  },
  {
   "source": [
    "## Listing Group Content\n",
    "\n",
    "Showing HDF5 objects in several of the file's groups:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Members of group \"/\":\n\t[<HDF5 dataset \"chunked\": shape (100, 200), type \"<i8\">, <HDF5 group \"/g1\" (1 members)>, <HDF5 group \"/several\" (2 members)>]\n\nMembers of group \"/g1\":\n\t[<HDF5 dataset \"contiguous\": shape (10,), type \"<i4\">]\n\nMembers of group \"/several\":\n\t[<HDF5 dataset \"compressed\": shape (250, 300), type \"<f4\">, <HDF5 group \"/several/levels\" (2 members)>]\n\nMembers of group \"/several/levels\":\n\t[<HDF5 group \"/several/levels/below\" (1 members)>, <HDF5 dataset \"unlimited\": shape (34, 76), type \"<f8\">]\n\n"
     ]
    }
   ],
   "source": [
    "for group in ('/', '/g1', '/several', '/several/levels'):\n",
    "    grp = f[group]\n",
    "    print(f'Members of group \"{grp.name}\":\\n\\t{list(grp.values())}\\n')"
   ]
  },
  {
   "source": [
    "## Reading Attributes\n",
    "\n",
    "Assigned attributes to any of the groups or datasets are accessible via the `.attrs` property:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Attributes of <HDF5 group \"/\" (3 members)>:\n\t[('attribute example 1', 'Example 1'), ('attribute example 2', 15.0)]\n\nAttributes of <HDF5 dataset \"chunked\": shape (100, 200), type \"<i8\">:\n\t[('description', 'Chunked dataset')]\n\nAttributes of <HDF5 dataset \"unlimited\": shape (34, 76), type \"<f8\">:\n\t[('description', 'Unlimited dataset')]\n\n"
     ]
    }
   ],
   "source": [
    "for path in ('/', '/chunked', '/several/levels/unlimited'):\n",
    "    obj = f[path]\n",
    "    print(f'Attributes of {obj}:\\n\\t{list(obj.attrs.items())}\\n')"
   ]
  },
  {
   "source": [
    "Reading an attribute is similar to accessing a dictionary value:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Unlimited dataset'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "f['/several/levels/unlimited'].attrs['description']"
   ]
  },
  {
   "source": [
    "## Reading Dataset Data\n",
    "\n",
    "h5py supports accessing dataset data in similar fashion to NumPy arrays. (The same also applies to writing dataset data but this is a very short tutorial!) The familiar NumPy slicing syntax is translated to HDF5 [hyperslab selections](https://portal.hdfgroup.org/display/HDF5/Reading+From+or+Writing+To+a+Subset+of+a+Dataset). Check out the h5py's [documentation](https://docs.h5py.org/en/2.10.0/high/dataset.html#reading-writing-data) for more information.\n",
    "\n",
    "Below are several examples of reading the `chunked` dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 dataset \"chunked\": shape (100, 200), type \"<i8\">"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "chnkd = f['/chunked']\n",
    "chnkd"
   ]
  },
  {
   "source": [
    "Note that the `chnkd` object **is not** a NumPy array itself but the result of slicing (reading) it is."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176],\n",
       "       [2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376],\n",
       "       [2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576],\n",
       "       [2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776],\n",
       "       [2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976],\n",
       "       [3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176],\n",
       "       [3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376],\n",
       "       [3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576],\n",
       "       [3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776],\n",
       "       [3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976]])"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "out = chnkd[10:20, 167:177]\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2167, 2170, 2173, 2176],\n",
       "       [2567, 2570, 2573, 2576],\n",
       "       [2967, 2970, 2973, 2976],\n",
       "       [3367, 3370, 3373, 3376],\n",
       "       [3767, 3770, 3773, 3776]])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "chnkd[10:20:2, 167:177:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[3170, 3172, 3176],\n",
       "       [3370, 3372, 3376],\n",
       "       [3570, 3572, 3576]])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "chnkd[15:18, [170, 172, 176]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "f['/g1/contiguous'][...]"
   ]
  },
  {
   "source": [
    "## Low-Level h5py API\n",
    "\n",
    "So far in the tutorial we featured the high-level h5py API. h5py also has a low-level API on which the high-level API is built upon. The low-level API consists of Python classes and their methods closely related to the HDF5 C API. Complete documentation for the low-level API is at http://api.h5py.org.\n",
    "\n",
    "The low-level API allows fine-grained control over all aspects of the HDF5 library. Finding an appropriate low-level class or method is very easy. For example, let's consider the [`H5Dset_extent()`](https://portal.hdfgroup.org/display/HDF5/H5D_SET_EXTENT) C function. It belongs to the HDF5 dataset (H5D) API. It's equivalent in the low-level h5py API is the [`h5py.h5d.set_extent()`](http://api.h5py.org/h5d.html#h5py.h5d.DatasetID.set_extent) method.\n",
    "\n",
    "Low-level API is available from all h5py high-level objects via the `.id` property. Using the `chnkd` dataset:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<h5py.h5d.DatasetID at 0x111ecb860>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "chnkd.id"
   ]
  },
  {
   "source": [
    "The `h5py.h5d.DatasetID` is the class with the methods from the HDF5 library's [H5D C API](https://portal.hdfgroup.org/display/HDF5/Datasets). For example, to find out the dataspace status of this dataset ([`H5Dget_space_status`](https://portal.hdfgroup.org/display/HDF5/H5D_GET_SPACE_STATUS)):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'SPACE_STATUS_ALLOCATED'"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "status = {h5py.h5d.SPACE_STATUS_NOT_ALLOCATED: 'SPACE_STATUS_NOT_ALLOCATED',\n",
    "          h5py.h5d.SPACE_STATUS_PART_ALLOCATED: 'SPACE_STATUS_PART_ALLOCATED',\n",
    "          h5py.h5d.SPACE_STATUS_ALLOCATED: 'SPACE_STATUS_ALLOCATED'}\n",
    "status[chnkd.id.get_space_status()]"
   ]
  },
  {
   "source": [
    "The HDF5 library's [identifier](https://portal.hdfgroup.org/display/HDF5/Using+Identifiers) related to the `chnkd` dataset is available from the `.id` property of `chnkd.id` object:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "360287970189639692"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "chnkd.id.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "source": [
    "# THE END"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
